# DEEPCRAFT Studio 5.3.2569+c38e822c721f137984639bf6e13e3974e71c734c
# Copyright Â© 2023- Imagimob AB, All Rights Reserved.
# 
# Generated at 04/22/2025 15:00:33 UTC. Any changes will be lost.
# 
# Layer                          Shape           Type       Function
# Sliding Window (data points)   [512]           float      dequeue
# Hamming smoothing              [512]           float      dequeue
# Real Discrete Fourier Transform [257,2]         float      dequeue
# Frobenius norm                 [257]           float      dequeue
# Mel Filterbank                 [40]            float      dequeue
# Add Constant                   [40]            float      dequeue
# Logarithm                      [40]            float      dequeue
# Clip                           [40]            float      dequeue
# Imagimob Speech Features       [40]            float      dequeue
# Sliding Window (data points)   [50,40]         float      dequeue
# Contextual Window (Sliding Window) [50,40]         float      dequeue
# 

import numpy as np
import enum

_K7 = np.empty((6352), dtype=np.int8)	# byte (8 bit) 
_K22 = np.empty((8608), dtype=np.int8)	# byte (8 bit) 
_K5 = np.empty((512), dtype=np.float32)	# float (32 bit) 
_K6 = np.empty((2), dtype=np.float32)	# float (32 bit) 
_K9 = np.empty((512), dtype=np.float32)	# float (32 bit) 
_K10 = np.empty((257,2), dtype=np.float32)	# float (32 bit) 
_K14 = np.empty((257), dtype=np.float32)	# float (32 bit) 
_K16 = np.empty((40), dtype=np.float32)	# float (32 bit) 
_K17 = np.empty((40), dtype=np.float32)	# float (32 bit) 
_K18 = np.empty((40), dtype=np.float32)	# float (32 bit) 
_K19 = np.empty((40), dtype=np.float32)	# float (32 bit) 

_K8 = np.array([7.99999982118606567383E-002, 8.00347700715065002441E-002, 8.01390856504440307617E-002, 8.03129225969314575195E-002, 8.05562585592269897461E-002, 8.08690562844276428223E-002, 8.12512710690498352051E-002, 8.17028358578681945801E-002, 8.22236984968185424805E-002, 8.28137621283531188965E-002, 8.34729522466659545898E-002, 8.42011570930480957031E-002, 8.49982723593711853027E-002, 8.58641862869262695312E-002, 8.67987498641014099121E-002, 8.78018364310264587402E-002, 8.88732895255088806152E-002, 9.00129452347755432129E-002, 9.12206321954727172852E-002, 9.24961641430854797363E-002, 9.38393622636795043945E-002, 9.52500030398368835449E-002, 9.67278927564620971680E-002, 9.82727929949760437012E-002, 9.98844802379608154297E-002, 1.01562708616256713867E-001, 1.03307217359542846680E-001, 1.05117753148078918457E-001, 1.06994032859802246094E-001, 1.08935780823230743408E-001, 1.10942699015140533447E-001, 1.13014481961727142334E-001, 1.15150824189186096191E-001, 1.17351390421390533447E-001, 1.19615860283374786377E-001, 1.21943883597850799561E-001, 1.24335117638111114502E-001, 1.26789182424545288086E-001, 1.29305735230445861816E-001, 1.31884366273880004883E-001, 1.34524703025817871094E-001, 1.37226343154907226562E-001, 1.39988884329795837402E-001, 1.42811894416809082031E-001, 1.45694956183433532715E-001, 1.48637622594833374023E-001, 1.51639461517333984375E-001, 1.54700025916099548340E-001, 1.57818824052810668945E-001, 1.60995423793792724609E-001, 1.64229303598403930664E-001, 1.67520001530647277832E-001, 1.70867025852203369141E-001, 1.74269840121269226074E-001, 1.77727952599525451660E-001, 1.81240841746330261230E-001, 1.84807971119880676270E-001, 1.88428804278373718262E-001, 1.92102774977684020996E-001, 1.95829346776008605957E-001, 1.99607968330383300781E-001, 2.03438028693199157715E-001, 2.07318991422653198242E-001, 2.11250245571136474609E-001, 2.15231195092201232910E-001, 2.19261258840560913086E-001, 2.23339796066284179688E-001, 2.27466225624084472656E-001, 2.31639891862869262695E-001, 2.35860183835029602051E-001, 2.40126460790634155273E-001, 2.44438067078590393066E-001, 2.48794361948966979980E-001, 2.53194689750671386719E-001, 2.57638365030288696289E-001, 2.62124747037887573242E-001, 2.66653120517730712891E-001, 2.71222829818725585938E-001, 2.75833189487457275391E-001, 2.80483454465866088867E-001, 2.85172998905181884766E-001, 2.89901047945022583008E-001, 2.94666886329650878906E-001, 2.99469828605651855469E-001, 3.04309159517288208008E-001, 3.09184104204177856445E-001, 3.14093947410583496094E-001, 3.19037944078445434570E-001, 3.24015349149703979492E-001, 3.29025387763977050781E-001, 3.34067344665527343750E-001, 3.39140444993972778320E-001, 3.44243884086608886719E-001, 3.49376946687698364258E-001, 3.54538798332214355469E-001, 3.59728723764419555664E-001, 3.64945888519287109375E-001, 3.70189517736434936523E-001, 3.75458806753158569336E-001, 3.80752980709075927734E-001, 3.86071234941482543945E-001, 3.91412764787673950195E-001, 3.96776765584945678711E-001, 4.02162402868270874023E-001, 4.07568901777267456055E-001, 4.12995398044586181641E-001, 4.18441087007522583008E-001, 4.23905193805694580078E-001, 4.29386824369430541992E-001, 4.34885174036026000977E-001, 4.40399438142776489258E-001, 4.45928752422332763672E-001, 4.51472282409667968750E-001, 4.57029193639755249023E-001, 4.62598651647567749023E-001, 4.68179821968078613281E-001, 4.73771840333938598633E-001, 4.79373872280120849609E-001, 4.84985053539276123047E-001, 4.90604579448699951172E-001, 4.96231555938720703125E-001, 5.01865148544311523438E-001, 5.07504522800445556641E-001, 5.13148784637451171875E-001, 5.18797159194946289062E-001, 5.24448692798614501953E-001, 5.30102610588073730469E-001, 5.35757958889007568359E-001, 5.41414022445678710938E-001, 5.47069847583770751953E-001, 5.52724599838256835938E-001, 5.58377444744110107422E-001, 5.64027488231658935547E-001, 5.69673895835876464844E-001, 5.75315833091735839844E-001, 5.80952405929565429688E-001, 5.86582779884338378906E-001, 5.92206180095672607422E-001, 5.97821652889251708984E-001, 6.03428363800048828125E-001, 6.09025478363037109375E-001, 6.14612162113189697266E-001, 6.20187580585479736328E-001, 6.25750899314880371094E-001, 6.31301224231719970703E-001, 6.36837720870971679688E-001, 6.42359614372253417969E-001, 6.47866010665893554688E-001, 6.53356134891510009766E-001, 6.58829092979431152344E-001, 6.64284110069274902344E-001, 6.69720292091369628906E-001, 6.75136923789978027344E-001, 6.80533051490783691406E-001, 6.85908019542694091797E-001, 6.91260874271392822266E-001, 6.96590840816497802734E-001, 7.01897144317626953125E-001, 7.07179009914398193359E-001, 7.12435543537139892578E-001, 7.17666089534759521484E-001, 7.22869694232940673828E-001, 7.28045701980590820312E-001, 7.33193218708038330078E-001, 7.38311588764190673828E-001, 7.43399977684020996094E-001, 7.48457551002502441406E-001, 7.53483653068542480469E-001, 7.58477509021759033203E-001, 7.63438284397125244141E-001, 7.68365323543548583984E-001, 7.73257791996002197266E-001, 7.78115034103393554688E-001, 7.82936215400695800781E-001, 7.87720739841461181641E-001, 7.92467772960662841797E-001, 7.97176659107208251953E-001, 8.01846623420715332031E-001, 8.06477010250091552734E-001, 8.11067163944244384766E-001, 8.15616250038146972656E-001, 8.20123732089996337891E-001, 8.24588835239410400391E-001, 8.29010903835296630859E-001, 8.33389341831207275391E-001, 8.37723374366760253906E-001, 8.42012405395507812500E-001, 8.46255719661712646484E-001, 8.50452780723571777344E-001, 8.54602932929992675781E-001, 8.58705520629882812500E-001, 8.62759888172149658203E-001, 8.66765439510345458984E-001, 8.70721638202667236328E-001, 8.74627828598022460938E-001, 8.78483414649963378906E-001, 8.82287800312042236328E-001, 8.86040449142456054688E-001, 8.89740824699401855469E-001, 8.93388271331787109375E-001, 8.96982312202453613281E-001, 9.00522410869598388672E-001, 9.04007971286773681641E-001, 9.07438516616821289062E-001, 9.10813510417938232422E-001, 9.14132416248321533203E-001, 9.17394757270812988281E-001, 9.20600056648254394531E-001, 9.23747837543487548828E-001, 9.26837563514709472656E-001, 9.29868817329406738281E-001, 9.32841122150421142578E-001, 9.35754060745239257812E-001, 9.38607156276702880859E-001, 9.41399991512298583984E-001, 9.44132089614868164062E-001, 9.46803152561187744141E-001, 9.49412703514099121094E-001, 9.51960325241088867188E-001, 9.54445660114288330078E-001, 9.56868350505828857422E-001, 9.59228038787841796875E-001, 9.61524367332458496094E-001, 9.63756918907165527344E-001, 9.65925395488739013672E-001, 9.68029499053955078125E-001, 9.70068871974945068359E-001, 9.72043275833129882812E-001, 9.73952293395996093750E-001, 9.75795745849609375000E-001, 9.77573335170745849609E-001, 9.79284703731536865234E-001, 9.80929672718048095703E-001, 9.82508003711700439453E-001, 9.84019458293914794922E-001, 9.85463738441467285156E-001, 9.86840665340423583984E-001, 9.88150060176849365234E-001, 9.89391684532165527344E-001, 9.90565419197082519531E-001, 9.91670966148376464844E-001, 9.92708265781402587891E-001, 9.93677079677581787109E-001, 9.94577348232269287109E-001, 9.95408892631530761719E-001, 9.96171593666076660156E-001, 9.96865272521972656250E-001, 9.97489929199218750000E-001, 9.98045384883880615234E-001, 9.98531639575958251953E-001, 9.98948514461517333984E-001, 9.99296009540557861328E-001, 9.99574065208435058594E-001, 9.99782681465148925781E-001, 9.99921739101409912109E-001, 9.99991297721862792969E-001, 9.99991297721862792969E-001, 9.99921739101409912109E-001, 9.99782681465148925781E-001, 9.99574065208435058594E-001, 9.99296009540557861328E-001, 9.98948514461517333984E-001, 9.98531639575958251953E-001, 9.98045384883880615234E-001, 9.97489929199218750000E-001, 9.96865272521972656250E-001, 9.96171593666076660156E-001, 9.95408892631530761719E-001, 9.94577348232269287109E-001, 9.93677079677581787109E-001, 9.92708265781402587891E-001, 9.91670966148376464844E-001, 9.90565419197082519531E-001, 9.89391684532165527344E-001, 9.88150060176849365234E-001, 9.86840665340423583984E-001, 9.85463738441467285156E-001, 9.84019458293914794922E-001, 9.82508003711700439453E-001, 9.80929672718048095703E-001, 9.79284703731536865234E-001, 9.77573335170745849609E-001, 9.75795745849609375000E-001, 9.73952293395996093750E-001, 9.72043275833129882812E-001, 9.70068871974945068359E-001, 9.68029499053955078125E-001, 9.65925395488739013672E-001, 9.63756918907165527344E-001, 9.61524367332458496094E-001, 9.59228038787841796875E-001, 9.56868350505828857422E-001, 9.54445660114288330078E-001, 9.51960325241088867188E-001, 9.49412703514099121094E-001, 9.46803152561187744141E-001, 9.44132089614868164062E-001, 9.41399991512298583984E-001, 9.38607156276702880859E-001, 9.35754060745239257812E-001, 9.32841122150421142578E-001, 9.29868817329406738281E-001, 9.26837563514709472656E-001, 9.23747837543487548828E-001, 9.20600056648254394531E-001, 9.17394757270812988281E-001, 9.14132416248321533203E-001, 9.10813510417938232422E-001, 9.07438516616821289062E-001, 9.04007971286773681641E-001, 9.00522410869598388672E-001, 8.96982312202453613281E-001, 8.93388271331787109375E-001, 8.89740824699401855469E-001, 8.86040449142456054688E-001, 8.82287800312042236328E-001, 8.78483414649963378906E-001, 8.74627828598022460938E-001, 8.70721638202667236328E-001, 8.66765439510345458984E-001, 8.62759888172149658203E-001, 8.58705520629882812500E-001, 8.54602932929992675781E-001, 8.50452780723571777344E-001, 8.46255719661712646484E-001, 8.42012405395507812500E-001, 8.37723374366760253906E-001, 8.33389341831207275391E-001, 8.29010903835296630859E-001, 8.24588835239410400391E-001, 8.20123732089996337891E-001, 8.15616250038146972656E-001, 8.11067163944244384766E-001, 8.06477010250091552734E-001, 8.01846623420715332031E-001, 7.97176659107208251953E-001, 7.92467772960662841797E-001, 7.87720739841461181641E-001, 7.82936215400695800781E-001, 7.78115034103393554688E-001, 7.73257791996002197266E-001, 7.68365323543548583984E-001, 7.63438284397125244141E-001, 7.58477509021759033203E-001, 7.53483653068542480469E-001, 7.48457551002502441406E-001, 7.43399977684020996094E-001, 7.38311588764190673828E-001, 7.33193218708038330078E-001, 7.28045701980590820312E-001, 7.22869694232940673828E-001, 7.17666089534759521484E-001, 7.12435543537139892578E-001, 7.07179009914398193359E-001, 7.01897144317626953125E-001, 6.96590840816497802734E-001, 6.91260874271392822266E-001, 6.85908019542694091797E-001, 6.80533051490783691406E-001, 6.75136923789978027344E-001, 6.69720292091369628906E-001, 6.64284110069274902344E-001, 6.58829092979431152344E-001, 6.53356134891510009766E-001, 6.47866010665893554688E-001, 6.42359614372253417969E-001, 6.36837720870971679688E-001, 6.31301224231719970703E-001, 6.25750899314880371094E-001, 6.20187580585479736328E-001, 6.14612162113189697266E-001, 6.09025478363037109375E-001, 6.03428363800048828125E-001, 5.97821652889251708984E-001, 5.92206180095672607422E-001, 5.86582779884338378906E-001, 5.80952405929565429688E-001, 5.75315833091735839844E-001, 5.69673895835876464844E-001, 5.64027488231658935547E-001, 5.58377444744110107422E-001, 5.52724599838256835938E-001, 5.47069847583770751953E-001, 5.41414022445678710938E-001, 5.35757958889007568359E-001, 5.30102610588073730469E-001, 5.24448692798614501953E-001, 5.18797159194946289062E-001, 5.13148784637451171875E-001, 5.07504522800445556641E-001, 5.01865148544311523438E-001, 4.96231555938720703125E-001, 4.90604579448699951172E-001, 4.84985053539276123047E-001, 4.79373872280120849609E-001, 4.73771840333938598633E-001, 4.68179821968078613281E-001, 4.62598651647567749023E-001, 4.57029193639755249023E-001, 4.51472282409667968750E-001, 4.45928752422332763672E-001, 4.40399438142776489258E-001, 4.34885174036026000977E-001, 4.29386824369430541992E-001, 4.23905193805694580078E-001, 4.18441087007522583008E-001, 4.12995398044586181641E-001, 4.07568901777267456055E-001, 4.02162402868270874023E-001, 3.96776765584945678711E-001, 3.91412764787673950195E-001, 3.86071234941482543945E-001, 3.80752980709075927734E-001, 3.75458806753158569336E-001, 3.70189517736434936523E-001, 3.64945888519287109375E-001, 3.59728723764419555664E-001, 3.54538798332214355469E-001, 3.49376946687698364258E-001, 3.44243884086608886719E-001, 3.39140444993972778320E-001, 3.34067344665527343750E-001, 3.29025387763977050781E-001, 3.24015349149703979492E-001, 3.19037944078445434570E-001, 3.14093947410583496094E-001, 3.09184104204177856445E-001, 3.04309159517288208008E-001, 2.99469828605651855469E-001, 2.94666886329650878906E-001, 2.89901047945022583008E-001, 2.85172998905181884766E-001, 2.80483454465866088867E-001, 2.75833189487457275391E-001, 2.71222829818725585938E-001, 2.66653120517730712891E-001, 2.62124747037887573242E-001, 2.57638365030288696289E-001, 2.53194689750671386719E-001, 2.48794361948966979980E-001, 2.44438067078590393066E-001, 2.40126460790634155273E-001, 2.35860183835029602051E-001, 2.31639891862869262695E-001, 2.27466225624084472656E-001, 2.23339796066284179688E-001, 2.19261258840560913086E-001, 2.15231195092201232910E-001, 2.11250245571136474609E-001, 2.07318991422653198242E-001, 2.03438028693199157715E-001, 1.99607968330383300781E-001, 1.95829346776008605957E-001, 1.92102774977684020996E-001, 1.88428804278373718262E-001, 1.84807971119880676270E-001, 1.81240841746330261230E-001, 1.77727952599525451660E-001, 1.74269840121269226074E-001, 1.70867025852203369141E-001, 1.67520001530647277832E-001, 1.64229303598403930664E-001, 1.60995423793792724609E-001, 1.57818824052810668945E-001, 1.54700025916099548340E-001, 1.51639461517333984375E-001, 1.48637622594833374023E-001, 1.45694956183433532715E-001, 1.42811894416809082031E-001, 1.39988884329795837402E-001, 1.37226343154907226562E-001, 1.34524703025817871094E-001, 1.31884366273880004883E-001, 1.29305735230445861816E-001, 1.26789182424545288086E-001, 1.24335117638111114502E-001, 1.21943883597850799561E-001, 1.19615860283374786377E-001, 1.17351390421390533447E-001, 1.15150824189186096191E-001, 1.13014481961727142334E-001, 1.10942699015140533447E-001, 1.08935780823230743408E-001, 1.06994032859802246094E-001, 1.05117753148078918457E-001, 1.03307217359542846680E-001, 1.01562708616256713867E-001, 9.98844802379608154297E-002, 9.82727929949760437012E-002, 9.67278927564620971680E-002, 9.52500030398368835449E-002, 9.38393622636795043945E-002, 9.24961641430854797363E-002, 9.12206321954727172852E-002, 9.00129452347755432129E-002, 8.88732895255088806152E-002, 8.78018364310264587402E-002, 8.67987498641014099121E-002, 8.58641862869262695312E-002, 8.49982723593711853027E-002, 8.42011570930480957031E-002, 8.34729522466659545898E-002, 8.28137621283531188965E-002, 8.22236984968185424805E-002, 8.17028358578681945801E-002, 8.12512710690498352051E-002, 8.08690562844276428223E-002, 8.05562585592269897461E-002, 8.03129225969314575195E-002, 8.01390856504440307617E-002, 8.00347700715065002441E-002, 7.99999982118606567383E-002, ]).reshape((512))
_K15 = np.array([9.00000000000000000000E+000, 1.10000000000000000000E+001, 1.30000000000000000000E+001, 1.50000000000000000000E+001, 1.70000000000000000000E+001, 1.90000000000000000000E+001, 2.10000000000000000000E+001, 2.30000000000000000000E+001, 2.60000000000000000000E+001, 2.90000000000000000000E+001, 3.10000000000000000000E+001, 3.40000000000000000000E+001, 3.70000000000000000000E+001, 4.10000000000000000000E+001, 4.40000000000000000000E+001, 4.80000000000000000000E+001, 5.20000000000000000000E+001, 5.60000000000000000000E+001, 6.00000000000000000000E+001, 6.40000000000000000000E+001, 6.90000000000000000000E+001, 7.40000000000000000000E+001, 7.90000000000000000000E+001, 8.50000000000000000000E+001, 9.10000000000000000000E+001, 9.70000000000000000000E+001, 1.03000000000000000000E+002, 1.10000000000000000000E+002, 1.18000000000000000000E+002, 1.25000000000000000000E+002, 1.33000000000000000000E+002, 1.42000000000000000000E+002, 1.51000000000000000000E+002, 1.60000000000000000000E+002, 1.70000000000000000000E+002, 1.80000000000000000000E+002, 1.91000000000000000000E+002, 2.03000000000000000000E+002, 2.15000000000000000000E+002, 2.28000000000000000000E+002, 2.42000000000000000000E+002, 2.56000000000000000000E+002, ]).reshape((42))

class ReturnStatus(enum.Enum): 
    RET_SUCCESS = 0
    RET_NODATA = -1
    RET_NOMEM = -2

class SlidingWindowTime:
    def __init__(self, input_size: int, window_count : int):
        self._data_buffer = []
        self._time_buffer = []
        self._data_buffer_size = input_size * window_count
        self._input_size = input_size
        self._window_count = window_count
            
    def enqueue(self, data, time):
    
        self._data_buffer.extend(data.flatten())

        if len(self._data_buffer) > self._data_buffer_size:
            return ReturnStatus.RET_NOMEM                

        self._time_buffer.extend([time.flatten().min(), time.flatten().max()])
        return ReturnStatus.RET_SUCCESS

    def dequeue(self, data_out, data_stride: int, time_out):
        data_window_size = np.prod(data_out.shape)
        if len(self._data_buffer) >= data_window_size:
            data_window = np.array(self._data_buffer[:data_window_size]).reshape(data_out.shape)
            np.copyto(data_out, data_window)
            del self._data_buffer[:(data_stride * self._input_size)]
           
            timestamp_count = 2 * self._window_count;
            time_window = np.array(self._time_buffer[:timestamp_count]).reshape(timestamp_count)
            time_out[0] = time_window.min()
            time_out[1] = time_window.max()
            del self._time_buffer[:2 * data_stride]
            
            return ReturnStatus.RET_SUCCESS

        return ReturnStatus.RET_NODATA

def hamming_mul(a, b, output):
    np.multiply(a, b, out=output)

def rfft(input, output, axis):
    result = np.fft.rfft(input, axis=-axis-1)                  # compute FFT
    np.stack((result.real, result.imag), axis=-1, out=output)  # flatten complex array (e.g. complex [4,3,6] to float [4,3,6,2])

def norm(input, axis, output):
    axis = len(input.shape) - axis - 1          # reverse order
    np.copyto(output, np.linalg.norm(input, axis=axis))

def mel(input, filter_points, output, nsize, htk):  
    if not htk:
        raise NotImplementedError("not implemented. htk must be true")

    filters = np.zeros((len(filter_points)-2, nsize), dtype=np.float32)
    for n in range(len(filter_points)-2):
        n0 = int(filter_points[n])
        n1 = int(filter_points[n + 1])
        n2 = int(filter_points[n + 2])
        filters[n, n0 : n1] = np.linspace(0, 1, n1 - n0, endpoint=False)
        filters[n, n1 : n2] = np.linspace(1, 0, n2 - n1, endpoint=False)

    np.dot(filters, input, out=output)

def addi(input, value, output):
    np.add(input, [value], out=output)

def log(input, output, base):
    log_data = np.log(input) 
    if base != 0:
        log_data /= np.log(base)
    np.copyto(output, log_data)

def clip(input, min, max, output):  
    np.clip(input, min, max, out=output)


class Model:
    def __init__(self):
        self._K7 = SlidingWindowTime(1, 512)
        self._K22 = SlidingWindowTime(40, 50)
        self.data_in_count = 1
        self.data_in_shape = (1)
        self.time_in_count = 1
        self.time_in_shape = (1)
        self.data_out_count = 2000
        self.data_out_shape = (50,40)
        self.time_out_count = 2
        self.time_out_shape = (2)
        self.api = 'queue'

    def enqueue(self, data_in : np.array, time_in : np.array):
        """
        Enqueue features. Returns SUCCESS (0) on success, else RET_NOMEM (-2) when low on memory.
        
        Parameters:
         data_in(float[1]): DESCRIPTION. [1]
         time_in(float[1]): DESCRIPTION. [1]
        """
        
        return_status = self._K7.enqueue(data_in, time_in)
        if return_status != ReturnStatus.RET_SUCCESS:
            return return_status
        
        return ReturnStatus.RET_SUCCESS

    def dequeue(self, data_out : np.array, time_out : np.array):
        """
        Dequeue features. RET_SUCCESS (0) on success, RET_NODATA (-1) if no data is available, RET_NOMEM (-2) on internal memory error
        
        Parameters:
         data_out(float[2000]): DESCRIPTION. [50,40]
         time_out(float[2]): DESCRIPTION. [2]
        """
        
        while True:
            return_status = self._K7.dequeue(_K5, 160, _K6)
            if return_status == ReturnStatus.RET_NODATA:
                break
            if return_status != ReturnStatus.RET_SUCCESS:
                return return_status
            
            hamming_mul(_K5, _K8, _K9)
            rfft(_K9, _K10, 0)
            norm(_K10, 0, _K14)
            mel(_K14, _K15, _K16, 257, True)
            addi(_K16, 1, _K17)
            log(_K17, _K18, 0)
            clip(_K18, 0, 4, _K19)
            return_status = self._K22.enqueue(_K19, _K6)
            if return_status == ReturnStatus.RET_NODATA:
                break
            if return_status != ReturnStatus.RET_SUCCESS:
                return return_status
            

        return_status = self._K22.dequeue(data_out.reshape([50,40]), 14, time_out.reshape([2]))
        if return_status != ReturnStatus.RET_SUCCESS:
            return return_status
        
        return ReturnStatus.RET_SUCCESS

